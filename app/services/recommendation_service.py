from typing import List, Dict, Any, Optional
import time
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor
import hashlib
import json

from app.models.pre_info import PreInfo
from app.schemas.spot import RecommendSpots
from app.services.llm_service import LLMService
from app.services.google_trends_service import GoogleTrendsService
from app.services.places_service import PlacesService
from app.services.vector_search_service import VectorSearchService
from app.services.scoring_service import ScoringService


class RecommendationService:
    """
    ã‚¹ãƒãƒƒãƒˆæ¨è–¦ã‚µãƒ¼ãƒ“ã‚¹
    ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³ã«å¾“ã£ã¦å¤šæ®µéšæ¨è–¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
    """

    def __init__(self):
        try:
            print("ğŸš€ RecommendationServiceåˆæœŸåŒ–é–‹å§‹...")

            # LLMã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
            print("ğŸ¤– LLMServiceåˆæœŸåŒ–ä¸­...")
            self.llm_service = LLMService()
            print("âœ… LLMServiceåˆæœŸåŒ–å®Œäº†")

            # Google Trendsã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
            print("ğŸ”¥ GoogleTrendsServiceåˆæœŸåŒ–ä¸­...")
            self.google_trends_service = GoogleTrendsService()
            print("âœ… GoogleTrendsServiceåˆæœŸåŒ–å®Œäº†")

            # Placesã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
            print("ğŸ—ºï¸ PlacesServiceåˆæœŸåŒ–ä¸­...")
            self.places_service = PlacesService()
            print("âœ… PlacesServiceåˆæœŸåŒ–å®Œäº†")

            # Vector Searchã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
            print("ğŸ¯ VectorSearchServiceåˆæœŸåŒ–ä¸­...")
            self.vector_search_service = VectorSearchService()
            print("âœ… VectorSearchServiceåˆæœŸåŒ–å®Œäº†")

            # Scoringã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
            print("ğŸ† ScoringServiceåˆæœŸåŒ–ä¸­...")
            self.scoring_service = ScoringService()
            print("âœ… ScoringServiceåˆæœŸåŒ–å®Œäº†")

            # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
            self._cache = {}  # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ
            self._executor = ThreadPoolExecutor(max_workers=8)  # ë” ë§ì€ ì›Œì»¤
            self._cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ TTL

            # ê°•í™”ëœ ë°°ì¹˜ ì„¤ì •
            self._max_keywords = 3  # 5ê°œ â†’ 3ê°œë¡œ ê°ì†Œ
            self._places_per_keyword = 10  # ë” ì ì€ ìˆ˜ë¡œ ìµœì í™”
            self._vector_limit = 50  # 80ê°œ â†’ 50ê°œ
            self._final_limit = 24  # 30ê°œ â†’ 24ê°œ
            self._batch_size = 30  # í° ë°°ì¹˜ í¬ê¸°

            print("âœ… RecommendationServiceåˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            print(f"âŒ RecommendationServiceåˆæœŸåŒ–å¤±æ•—: {str(e)}")
            # åˆæœŸåŒ–å¤±æ•—ã—ã¦ã‚‚ã‚µãƒ¼ãƒ“ã‚¹ã¯ç¶™ç¶šå®Ÿè¡Œ
            self.llm_service = None
            self.google_trends_service = None
            self.places_service = None
            self.vector_search_service = None
            self.scoring_service = None

    def _get_cache_key(self, pre_info: PreInfo) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        cache_data = {
            "region": pre_info.region,
            "atmosphere": pre_info.atmosphere,
            "budget": pre_info.budget,
            "participants_count": pre_info.participants_count,
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_string.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        if cache_key in self._cache:
            cached_data, timestamp = self._cache[cache_key]
            if time.time() - timestamp < self._cache_ttl:
                print(f"ğŸ’¾ ìºì‹œ íˆíŠ¸: {cache_key[:8]}...")
                return cached_data
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self._cache[cache_key]
                print(f"ğŸ—‘ï¸ ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {cache_key[:8]}...")
        return None

    def _save_to_cache(self, cache_key: str, data: Dict[str, Any]) -> None:
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        self._cache[cache_key] = (data, time.time())
        print(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_key[:8]}...")

        # ìºì‹œ í¬ê¸° ê´€ë¦¬ (ìµœëŒ€ 100ê°œ)
        if len(self._cache) > 100:
            # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ 1ê°œ ì‚­ì œ
            oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k][1])
            del self._cache[oldest_key]
            print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ìºì‹œ ì‚­ì œ: {oldest_key[:8]}...")

    async def recommend_spots_from_pre_info(self, pre_info: PreInfo) -> Dict[str, Any]:
        """
        ê·¹ë„ë¡œ ìµœì í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ (ë³‘ë ¬ + ë°°ì¹˜ ìµœì í™”)
        """
        start_time = time.time()

        # ìºì‹œ í‚¤ ìƒì„± ë° ì¡°íšŒ
        cache_key = self._get_cache_key(pre_info)
        cached_result = self._get_from_cache(cache_key)

        if cached_result:
            cache_time_ms = int((time.time() - start_time) * 1000)
            cached_result["processing_time_ms"] = cache_time_ms
            cached_result["from_cache"] = True
            print(f"âš¡ ìºì‹œ íˆíŠ¸ - ì¦‰ì‹œ ë°˜í™˜: {cache_time_ms}ms")
            return cached_result

        # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        processing_metadata = {
            "total_spots_found": 0,
            "api_calls_made": 0,
            "super_optimization_applied": True,
            "processing_steps": [],
            "from_cache": False,
        }

        try:
            print("ğŸš€ SUPER ìµœì í™” ëª¨ë“œ ì‹œì‘!")

            # ğŸ”¥ MEGA PHASE: ëª¨ë“  ì‘ì—…ì„ ìµœëŒ€í•œ ë³‘ë ¬ë¡œ
            mega_start = time.time()

            # ë™ì‹œ ì‹¤í–‰í•  ì‘ì—…ë“¤
            tasks = []

            # Task 1: LLM í‚¤ì›Œë“œ ìƒì„± (ë¹„ë™ê¸°)
            keywords_task = self._generate_keywords_optimized(pre_info)
            tasks.append(("keywords", keywords_task))

            # Task 2: ê¸°ë³¸ Places ê²€ìƒ‰ (ë³‘ë ¬ ì¤€ë¹„)
            basic_search_task = self._prepare_basic_search(pre_info)
            tasks.append(("basic_search", basic_search_task))

            # Task 3: Vector ëª¨ë¸ ì¤€ë¹„ (ë°±ê·¸ë¼ìš´ë“œ)
            vector_prep_task = self._prepare_vector_service()
            tasks.append(("vector_prep", vector_prep_task))

            print(f"ğŸ”¥ {len(tasks)}ê°œ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘...")

            # ëª¨ë“  ì‘ì—… ë™ì‹œ ì‹¤í–‰
            results = await asyncio.gather(
                *[task[1] for task in tasks], return_exceptions=True
            )

            # ê²°ê³¼ ì •ë¦¬
            keywords = (
                results[0]
                if not isinstance(results[0], Exception)
                else ["ë°”ë¥´ì…€ë¡œë‚˜ ì¡°ìš©í•œ ì¥ì†Œ", "ë°”ë¥´ì…€ë¡œë‚˜ ê³µì›", "ë°”ë¥´ì…€ë¡œë‚˜ ìˆ˜ë„ì›"]
            )
            basic_places = results[1] if not isinstance(results[1], Exception) else []
            vector_ready = results[2] if not isinstance(results[2], Exception) else True

            mega_phase1_time = (time.time() - mega_start) * 1000
            processing_metadata["processing_steps"].append(
                f"MegaPhase1: {mega_phase1_time:.0f}ms"
            )
            print(f"âœ… MEGA PHASE 1 ì™„ë£Œ: {mega_phase1_time:.0f}ms")

            # ğŸš€ MEGA PHASE 2: Places API í­ë°œì  ë³‘ë ¬ ì²˜ë¦¬
            phase2_start = time.time()

            # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ + ê¸°ë³¸ ê²€ìƒ‰ ê²°í•©
            all_search_tasks = []

            # í‚¤ì›Œë“œë³„ ë³‘ë ¬ ê²€ìƒ‰ (ìµœì í™”ëœ ë²„ì „ ì‚¬ìš©)
            for keyword in keywords[: self._max_keywords]:
                if self.places_service:
                    search_task = self.places_service.text_search_optimized(
                        keyword, pre_info.region, max_results=60
                    )
                    all_search_tasks.append(search_task)

            # ëª¨ë“  ê²€ìƒ‰ ë™ì‹œ ì‹¤í–‰
            if all_search_tasks:
                search_results = await asyncio.gather(
                    *all_search_tasks, return_exceptions=True
                )

                all_place_ids = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set ì‚¬ìš©
                for result in search_results:
                    if not isinstance(result, Exception) and result:
                        all_place_ids.update(result[: self._places_per_keyword])

                place_ids = list(all_place_ids)[: self._batch_size * 2]  # ìµœëŒ€ 60ê°œ
            else:
                place_ids = [f"fallback_place_{i}" for i in range(30)]

            processing_metadata["api_calls_made"] += len(all_search_tasks)

            # ë°°ì¹˜ë³„ Details ê°€ì ¸ì˜¤ê¸° (ìš¸íŠ¸ë¼ ë³‘ë ¬)
            place_details = await self._get_place_details_ultra_optimized(place_ids)
            processing_metadata["api_calls_made"] += len(place_ids)  # ì‹¤ì œ API í˜¸ì¶œ ìˆ˜
            processing_metadata["total_spots_found"] = len(place_details)

            phase2_time = (time.time() - phase2_start) * 1000
            processing_metadata["processing_steps"].append(
                f"MegaPhase2: {phase2_time:.0f}ms"
            )
            print(f"âœ… MEGA PHASE 2 ì™„ë£Œ: {phase2_time:.0f}ms")

            # ğŸ¯ MEGA PHASE 3: Vector + LLM + Scoring ì´ˆë³‘ë ¬ ì²˜ë¦¬
            phase3_start = time.time()

            # ë™ì‹œ ì‹¤í–‰: Vector Search + LLM ì¤€ë¹„
            vector_task = self._vector_search_mega_optimized(pre_info, place_details)

            # Vector Search ì™„ë£Œ í›„ LLM + Scoring ë³‘ë ¬
            vector_candidates = await vector_task
            processing_metadata["api_calls_made"] += 1

            # LLMê³¼ ê¸°ë³¸ ìŠ¤ì½”ì–´ë§ì„ ë™ì‹œì—
            llm_task = self._llm_rerank_ultra_fast(vector_candidates, pre_info)
            basic_scoring_task = self._basic_scoring_parallel(
                vector_candidates, pre_info
            )

            llm_result, basic_scores = await asyncio.gather(
                llm_task, basic_scoring_task, return_exceptions=True
            )

            # ê²°ê³¼ ê²°í•© (LLM ì„±ê³µ ì‹œ ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìŠ¤ì½”ì–´ë§)
            if not isinstance(llm_result, Exception):
                final_spots = llm_result[: self._final_limit]
            else:
                final_spots = (
                    basic_scores[: self._final_limit]
                    if not isinstance(basic_scores, Exception)
                    else vector_candidates[: self._final_limit]
                )

            processing_metadata["api_calls_made"] += 1

            phase3_time = (time.time() - phase3_start) * 1000
            processing_metadata["processing_steps"].append(
                f"MegaPhase3: {phase3_time:.0f}ms"
            )
            print(f"âœ… MEGA PHASE 3 ì™„ë£Œ: {phase3_time:.0f}ms")

            # ğŸ† ìµœì¢… ë³€í™˜ (ì´ˆê³ ì†)
            format_start = time.time()
            final_recommendations = self._format_spots_ultra_fast(final_spots)
            format_time = (time.time() - format_start) * 1000
            processing_metadata["processing_steps"].append(
                f"Format: {format_time:.0f}ms"
            )

            # ì´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time_ms = int((time.time() - start_time) * 1000)

            # ì„±ëŠ¥ ë¦¬í¬íŠ¸
            print("ğŸš€ SUPER ìµœì í™” ê²°ê³¼:")
            print(f"  - ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time_ms}ms")
            print(f"  - ë‹¨ê³„ë³„ ì‹œê°„: {processing_metadata['processing_steps']}")
            print(f"  - API í˜¸ì¶œ ìµœì í™”: {processing_metadata['api_calls_made']}íšŒ")
            print(f"  - ì¥ì†Œ ë°œê²¬: {processing_metadata['total_spots_found']}ê°œ")
            print(f"  - ìµœì¢… ì¶”ì²œ: {len(final_recommendations)}ê°œ ì‹œê°„ëŒ€")

            # ì´ˆê¸° ê°€ì¤‘ì¹˜ (ê°„ë‹¨í•œ ê¸°ë³¸ê°’)
            initial_weights = {
                "price": 0.7,
                "rating": 0.5,
                "congestion": 0.8,
                "similarity": 0.9,
            }

            # ìµœì¢… ê²°ê³¼ ìƒì„±
            result = {
                "rec_spot_id": f"rec_{int(datetime.now().timestamp())}",
                "recommend_spots": final_recommendations,
                "processing_time_ms": processing_time_ms,
                "keywords_generated": keywords,
                "hot_keywords": keywords,  # ê°„ì†Œí™”
                "initial_weights": initial_weights,
                **processing_metadata,
            }

            # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            self._save_to_cache(cache_key, result.copy())

            return result

        except Exception as e:
            processing_time_ms = int((time.time() - start_time) * 1000)
            raise Exception(
                f"SUPER ìµœì í™” ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {str(e)} (ì²˜ë¦¬ì‹œê°„: {processing_time_ms}ms)"
            )

    async def _generate_keywords_optimized(self, pre_info: PreInfo) -> List[str]:
        """ìµœì í™”ëœ í‚¤ì›Œë“œ ìƒì„± (ê°œìˆ˜ ê°ì†Œ)"""
        if self.llm_service is None:
            return [
                f"{pre_info.region} {pre_info.atmosphere}",
                f"{pre_info.region} ê³µì›",
                f"{pre_info.region} ì¹´í˜",
            ]

        try:
            keywords, _ = await self.llm_service.generate_keywords_and_weights(pre_info)
            return keywords[: self._max_keywords]  # 3ê°œë§Œ ì‚¬ìš©
        except:
            return [
                f"{pre_info.region} {pre_info.atmosphere}",
                f"{pre_info.region} ëª…ì†Œ",
            ]

    async def _prepare_basic_search(self, pre_info: PreInfo) -> List[str]:
        """ê¸°ë³¸ ê²€ìƒ‰ ì¤€ë¹„ (ë°±ê·¸ë¼ìš´ë“œ)"""
        # ì¼ë°˜ì ì¸ ì¥ì†Œ í‚¤ì›Œë“œ
        basic_keywords = [f"{pre_info.region} ê´€ê´‘", f"{pre_info.region} ëª…ì†Œ"]
        return basic_keywords

    async def _prepare_vector_service(self) -> bool:
        """Vector ì„œë¹„ìŠ¤ ì¤€ë¹„"""
        # Vector ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
        return self.vector_search_service is not None

    async def _get_place_details_ultra_optimized(
        self, place_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """ğŸš€ ìš¸íŠ¸ë¼ ìµœì í™”ëœ Places Details (ëŒ€ìš©ëŸ‰ ë³‘ë ¬ ë°°ì¹˜)"""
        if self.places_service is None:
            print("âš ï¸ PlacesService ì—†ìŒ. ìš¸íŠ¸ë¼ Fallback")
            return [
                {
                    "place_id": pid,
                    "name": f"ì¥ì†Œ_{i+1}",
                    "rating": 4.0 + (i % 10) * 0.1,
                    "address": "ì£¼ì†Œ ì •ë³´",
                    "lat": 41.3851 + (i * 0.001),
                    "lng": 2.1734 + (i * 0.001),
                    "price_level": (i % 4) + 1,
                    "types": ["establishment"],
                }
                for i, pid in enumerate(place_ids[:60])  # ë” ë§ì€ fallback
            ]

        try:
            print(f"ğŸš€ ìš¸íŠ¸ë¼ ë°°ì¹˜ Details: {len(place_ids)}ê°œ")

            # ìš¸íŠ¸ë¼ ë°°ì¹˜ ì²˜ë¦¬ (20ê°œì”© ë³‘ë ¬)
            place_details = await self.places_service.get_place_details_ultra_batch(
                place_ids, batch_size=20
            )

            print(f"âœ… ìš¸íŠ¸ë¼ ë°°ì¹˜ Details ì™„ë£Œ: {len(place_details)}ê°œ")
            return place_details[:60]  # ìµœëŒ€ 60ê°œë¡œ í™•ì¥

        except Exception as e:
            print(f"âŒ ìš¸íŠ¸ë¼ ë°°ì¹˜ Details ì‹¤íŒ¨: {str(e)}")
            # ê°„ë‹¨í•œ fallback ë°ì´í„° ë°˜í™˜
            return [
                {
                    "place_id": pid,
                    "name": f"Fallback_ì¥ì†Œ_{i+1}",
                    "rating": 4.0,
                    "address": "ì„ì‹œ ì£¼ì†Œ",
                    "lat": 41.3851 + (i * 0.001),
                    "lng": 2.1734 + (i * 0.001),
                    "price_level": 2,
                    "types": ["establishment"],
                }
                for i, pid in enumerate(place_ids[:30])
            ]

    async def _vector_search_mega_optimized(
        self, pre_info: PreInfo, places: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ë©”ê°€ ìµœì í™”ëœ Vector Search"""
        if self.vector_search_service is None:
            print("âš ï¸ Vector Search ì—†ìŒ. ë¹ ë¥¸ ì„ ë³„")
            return places[: self._vector_limit]

        try:
            # CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self._executor,
                lambda: self._vector_search_cpu_intensive(pre_info, places),
            )

            print(f"âœ… ë©”ê°€ Vector Search ì™„ë£Œ: {len(result)}ê°œ")
            return result[: self._vector_limit]
        except Exception as e:
            print(f"âŒ Vector Search ì‹¤íŒ¨: {str(e)}")
            return places[: self._vector_limit]

    def _vector_search_cpu_intensive(self, pre_info, places):
        """CPU ì§‘ì•½ì  Vector Search (ë³„ë„ ìŠ¤ë ˆë“œ)"""
        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì‹¤ì œë¡œëŠ” Sentence Transformer ì‚¬ìš©)
        scored_places = []
        query = f"{pre_info.atmosphere} {pre_info.region}"

        for place in places:
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë§¤ì¹­ ì ìˆ˜
            name = place.get("name", "")
            score = len(set(query.lower().split()) & set(name.lower().split()))
            place["similarity_score"] = score
            scored_places.append(place)

        # ì ìˆ˜ë³„ ì •ë ¬
        return sorted(
            scored_places, key=lambda x: x.get("similarity_score", 0), reverse=True
        )

    async def _llm_rerank_ultra_fast(
        self, candidates: List[Dict], pre_info: PreInfo
    ) -> List[Dict]:
        """ì´ˆê³ ì† LLM ì¬ë­í‚¹"""
        if self.llm_service is None:
            print("âš ï¸ LLM ì—†ìŒ. ë¹ ë¥¸ ì¬ë­í‚¹")
            return candidates[:40]

        try:
            # LLM ì¬ë­í‚¹ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            reranked, _ = await asyncio.wait_for(
                self.llm_service.rerank_and_adjust_weights(candidates, {}, pre_info),
                timeout=10.0,  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
            )
            return reranked[:40]
        except:
            print("âš ï¸ LLM íƒ€ì„ì•„ì›ƒ. ê¸°ë³¸ ì¬ë­í‚¹ ì‚¬ìš©")
            return candidates[:40]

    async def _basic_scoring_parallel(
        self, candidates: List[Dict], pre_info: PreInfo
    ) -> List[Dict]:
        """ë³‘ë ¬ ê¸°ë³¸ ìŠ¤ì½”ì–´ë§ (LLM ë°±ì—…ìš©)"""
        # CPU ì§‘ì•½ì  ìŠ¤ì½”ì–´ë§ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ
        loop = asyncio.get_event_loop()

        try:
            scored = await loop.run_in_executor(
                self._executor,
                lambda: self._calculate_basic_scores(candidates, pre_info),
            )
            return scored[:40]
        except:
            return candidates[:40]

    def _calculate_basic_scores(self, candidates: List[Dict], pre_info) -> List[Dict]:
        """ê¸°ë³¸ ìŠ¤ì½”ì–´ ê³„ì‚° (CPU ì§‘ì•½ì )"""
        for candidate in candidates:
            rating = candidate.get("rating", 3.5)
            price_level = candidate.get("price_level", 2)

            # ê°„ë‹¨í•œ ìŠ¤ì½”ì–´ë§
            rating_score = rating / 5.0
            price_score = 1.0 - (price_level - 1) / 4.0
            final_score = rating_score * 0.6 + price_score * 0.4

            candidate["final_score"] = final_score

        return sorted(candidates, key=lambda x: x.get("final_score", 0), reverse=True)

    def _format_spots_ultra_fast(
        self, spots: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ì´ˆê³ ì† ìŠ¤íŒŸ í¬ë§·íŒ…"""
        if not spots:
            return []

        # 3ê°œ ì‹œê°„ëŒ€ë¡œ ê· ë“± ë¶„ë°°
        spots_per_slot = len(spots) // 3
        remainder = len(spots) % 3

        time_slots = [
            ("åˆå‰", spots[: spots_per_slot + (1 if remainder > 0 else 0)]),
            (
                "åˆå¾Œ",
                spots[
                    spots_per_slot
                    + (1 if remainder > 0 else 0) : 2 * spots_per_slot
                    + (2 if remainder > 1 else 1 if remainder > 0 else 0)
                ],
            ),
            (
                "å¤œ",
                spots[
                    2 * spots_per_slot
                    + (2 if remainder > 1 else 1 if remainder > 0 else 0) :
                ],
            ),
        ]

        formatted_spots = []
        for time_slot, slot_spots in time_slots:
            if slot_spots:
                formatted_spots.append(
                    {
                        "time_slot": time_slot,
                        "spots": [
                            self._convert_to_spot_schema_fast(spot, idx)
                            for idx, spot in enumerate(slot_spots)
                        ],
                    }
                )

        return formatted_spots

    def _convert_to_spot_schema_fast(
        self, place_data: Dict[str, Any], index: int
    ) -> Dict[str, Any]:
        """ê³ ì† ìŠ¤íŒŸ ìŠ¤í‚¤ë§ˆ ë³€í™˜"""
        lat = place_data.get("lat", 41.3851)
        lng = place_data.get("lng", 2.1734)
        
        # Google Mapã«æŠ•ç¨¿ã•ã‚ŒãŸå†™çœŸã®URLã‚’å–å¾—ï¼ˆæœ€åˆã®1æšï¼‰
        photos = place_data.get("photos", [])
        google_map_image_url = photos[0] if photos else None
        
        return {
            "spot_id": place_data.get("place_id", f"spot_{index}"),
            "longitude": lng,
            "latitude": lat,
            "recommendation_reason": f"{place_data.get('name', 'å ´æ‰€')}ã¯è©•ä¾¡ {place_data.get('rating', 4.0):.1f}ã§ãŠã™ã™ã‚ã§ã™ã€‚",
            "details": {
                "name": place_data.get("name", f"ì¥ì†Œ_{index}"),
                "congestion": [40 + (i * 3) % 50 for i in range(24)],  # ê°„ë‹¨í•œ í˜¼ì¡ë„
                "business_hours": {
                    day: {"open_time": "09:00:00", "close_time": "18:00:00"}
                    for day in [
                        "MONDAY",
                        "TUESDAY",
                        "WEDNESDAY",
                        "THURSDAY",
                        "FRIDAY",
                        "SATURDAY",
                        "SUNDAY",
                        "HOLIDAY",
                    ]
                },
                "price": place_data.get("price_level", 2) * 1000,
            },
            "google_map_image_url": google_map_image_url,
            "website_url": place_data.get("website", None),
            "selected": False,
        }
