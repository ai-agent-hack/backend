import asyncio
import numpy as np
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
import os
from datetime import datetime

from app.models.pre_info import PreInfo


class VectorSearchService:
    """
    Vector Search ã‚µãƒ¼ãƒ“ã‚¹
    ãƒ¦ãƒ¼ã‚¶ãƒ¼æ—…è¡Œæƒ…å ±ã¨å ´æ‰€æƒ…å ±ã®æ„å‘³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    """

    def __init__(self):
        try:
            print("ğŸ¯ VectorSearchServiceåˆæœŸåŒ–é–‹å§‹...")

            # Sentence Transformer ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            # å¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ (æ—¥æœ¬èªãƒ»éŸ“å›½èªãƒ»è‹±èªã‚µãƒãƒ¼ãƒˆ)
            model_name = "paraphrase-multilingual-MiniLM-L12-v2"
            print(f"ğŸ“¡ Sentence Transformerãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {model_name}")

            self.model = SentenceTransformer(model_name)
            print("âœ… VectorSearchServiceåˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            print(f"âŒ VectorSearchServiceåˆæœŸåŒ–å¤±æ•—: {str(e)}")
            self.model = None

    async def find_similar_places(
        self, pre_info: PreInfo, places: List[Dict[str, Any]], limit: int = 80
    ) -> List[Dict[str, Any]]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼æ—…è¡Œæƒ…å ±ã¨é¡ä¼¼åº¦ã®é«˜ã„å ´æ‰€ã‚’é¸åˆ¥

        Args:
            pre_info: ãƒ¦ãƒ¼ã‚¶ãƒ¼æ—…è¡Œæƒ…å ±
            places: å ´æ‰€è©³ç´°ãƒªã‚¹ãƒˆ
            limit: è¿”å´ã™ã‚‹æœ€å¤§å ´æ‰€æ•°

        Returns:
            é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸå ´æ‰€ãƒªã‚¹ãƒˆ
        """
        if not self.model or not places:
            print("âš ï¸ VectorSearchServiceåˆ©ç”¨ä¸å¯ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨")
            return places[:limit]

        try:
            print(f"ğŸ¯ Vector Searché–‹å§‹: {len(places)}å€‹ã‹ã‚‰{limit}å€‹é¸åˆ¥")

            # Step 1: ãƒ¦ãƒ¼ã‚¶ãƒ¼æ—…è¡Œæƒ…å ±ã‹ã‚‰ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            user_query = self._create_user_query(pre_info)
            print(f"ğŸ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª: {user_query}")

            # Step 2: å„å ´æ‰€ã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            place_texts = []
            for place in places:
                place_text = self._create_place_text(place)
                place_texts.append(place_text)

            print(f"ğŸ—ï¸ å ´æ‰€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: {len(place_texts)}å€‹")

            # Step 3: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ (ä¸¦åˆ—å‡¦ç†å¯¾å¿œ)
            user_embedding = await self._get_embedding(user_query)
            place_embeddings = await self._get_embeddings_batch(place_texts)

            # Step 4: é¡ä¼¼åº¦è¨ˆç®—
            similarities = []
            for i, place_embedding in enumerate(place_embeddings):
                similarity = self._cosine_similarity(user_embedding, place_embedding)
                similarities.append((i, similarity))

            # Step 5: é¡ä¼¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
            similarities.sort(key=lambda x: x[1], reverse=True)

            # Step 6: ä¸Šä½limitå€‹é¸æŠ
            selected_places = []
            for i, (place_idx, similarity) in enumerate(similarities[:limit]):
                place = places[place_idx].copy()
                place["similarity_score"] = float(
                    similarity
                )  # numpy float -> Python float
                place["similarity_rank"] = i + 1
                selected_places.append(place)

            print(f"âœ… Vector Searchå®Œäº†: {len(selected_places)}å€‹é¸åˆ¥")
            top_scores = [p.get("similarity_score", 0) for p in selected_places[:5]]
            print(f"ğŸ† ãƒˆãƒƒãƒ—5é¡ä¼¼åº¦: {[f'{score:.3f}' for score in top_scores]}")

            return selected_places

        except Exception as e:
            print(f"âŒ Vector Searchå¤±æ•—: {str(e)}")
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®ãƒªã‚¹ãƒˆè¿”å´")
            return places[:limit]

    def _create_user_query(self, pre_info: PreInfo) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼æ—…è¡Œæƒ…å ±ã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        """
        parts = []

        # åœ°åŸŸæƒ…å ±
        if pre_info.region:
            parts.append(f"åœ°åŸŸ: {pre_info.region}")

        # äºˆç®—æƒ…å ±
        if pre_info.budget_per_person:
            budget_text = f"äºˆç®—: {pre_info.budget_per_person:,}å††"
            if pre_info.budget_per_person <= 50000:
                budget_text += " (ç¯€ç´„å¿—å‘)"
            elif pre_info.budget_per_person >= 100000:
                budget_text += " (é«˜ç´šå¿—å‘)"
            parts.append(budget_text)

        # äººæ•°æƒ…å ±
        if pre_info.participants_count:
            group_type = ""
            if pre_info.participants_count == 1:
                group_type = "ä¸€äººæ—…"
            elif pre_info.participants_count == 2:
                group_type = "ã‚«ãƒƒãƒ—ãƒ«æ—…è¡Œ"
            elif pre_info.participants_count <= 4:
                group_type = "å°ã‚°ãƒ«ãƒ¼ãƒ—æ—…è¡Œ"
            else:
                group_type = "å¤§ã‚°ãƒ«ãƒ¼ãƒ—æ—…è¡Œ"
            parts.append(f"äººæ•°: {pre_info.participants_count}äºº ({group_type})")

        # æœŸé–“æƒ…å ±
        if pre_info.start_date and pre_info.end_date:
            import datetime

            start = datetime.datetime.fromisoformat(
                pre_info.start_date.replace("Z", "+00:00")
            )
            end = datetime.datetime.fromisoformat(
                pre_info.end_date.replace("Z", "+00:00")
            )
            duration = (end - start).days + 1
            parts.append(f"æœŸé–“: {duration}æ—¥é–“")

        query = " ".join(parts)
        return query

    def _create_place_text(self, place: Dict[str, Any]) -> str:
        """
        å ´æ‰€æƒ…å ±ã‹ã‚‰èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        """
        parts = []

        # åå‰
        if place.get("name"):
            parts.append(f"åå‰: {place['name']}")

        # ä½æ‰€
        if place.get("address"):
            parts.append(f"ä½æ‰€: {place['address']}")

        # è©•ä¾¡
        if place.get("rating"):
            rating_text = f"è©•ä¾¡: {place['rating']}ç‚¹"
            if place.get("ratings_total"):
                rating_text += f" ({place['ratings_total']}ä»¶)"
            parts.append(rating_text)

        # ä¾¡æ ¼ãƒ¬ãƒ™ãƒ«
        if place.get("price_level"):
            price_levels = {1: "æ ¼å®‰", 2: "ãƒªãƒ¼ã‚ºãƒŠãƒ–ãƒ«", 3: "ä¸­ç´š", 4: "é«˜ç´š"}
            price_text = price_levels.get(place["price_level"], "ä¾¡æ ¼ä¸æ˜")
            parts.append(f"ä¾¡æ ¼å¸¯: {price_text}")

        # ã‚«ãƒ†ã‚´ãƒª
        if place.get("types") and isinstance(place["types"], list):
            # Google Places APIã®ã‚¿ã‚¤ãƒ—ã‚’æ—¥æœ¬èªã«å¤‰æ›
            type_translations = {
                "restaurant": "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³",
                "tourist_attraction": "è¦³å…‰åœ°",
                "lodging": "å®¿æ³Šæ–½è¨­",
                "shopping_mall": "ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ«",
                "museum": "åšç‰©é¤¨",
                "park": "å…¬åœ’",
                "cafe": "ã‚«ãƒ•ã‚§",
                "bar": "ãƒãƒ¼",
                "establishment": "æ–½è¨­",
            }

            translated_types = []
            for place_type in place["types"][:3]:  # æœ€å¤§3å€‹ã¾ã§
                translated = type_translations.get(place_type, place_type)
                translated_types.append(translated)

            if translated_types:
                parts.append(f"ã‚«ãƒ†ã‚´ãƒª: {', '.join(translated_types)}")

        text = " ".join(parts)
        return text

    async def _get_embedding(self, text: str) -> np.ndarray:
        """
        å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
        """
        if not self.model:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«
            return np.random.rand(384)  # MiniLM-L12-v2ã®æ¬¡å…ƒæ•°

        try:
            # éåŒæœŸå‡¦ç†å¯¾å¿œ (CPUé›†ç´„çš„ãªã®ã§åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ)
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(
                None, lambda: self.model.encode([text], convert_to_numpy=True)[0]
            )
            return embedding

        except Exception as e:
            print(f"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•—: {str(e)}")
            return np.random.rand(384)

    async def _get_embeddings_batch(self, texts: List[str]) -> List[np.ndarray]:
        """
        è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ä¸€æ‹¬å–å¾—
        """
        if not self.model:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ã‚¯ãƒˆãƒ«
            return [np.random.rand(384) for _ in texts]

        try:
            # ãƒãƒƒãƒå‡¦ç†ã§åŠ¹ç‡åŒ–
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(
                None,
                lambda: self.model.encode(
                    texts, convert_to_numpy=True, show_progress_bar=False
                ),
            )
            return list(embeddings)

        except Exception as e:
            print(f"âŒ ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•—: {str(e)}")
            return [np.random.rand(384) for _ in texts]

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        """
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«æ­£è¦åŒ–
            vec1_norm = vec1 / np.linalg.norm(vec1)
            vec2_norm = vec2 / np.linalg.norm(vec2)

            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            similarity = np.dot(vec1_norm, vec2_norm)

            # -1 ~ 1 ã®ç¯„å›²ã‚’ 0 ~ 1 ã«å¤‰æ›
            similarity = (similarity + 1) / 2

            return float(similarity)

        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦è¨ˆç®—å¤±æ•—: {str(e)}")
            return 0.5  # ä¸­ç«‹å€¤
